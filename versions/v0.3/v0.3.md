# V0.3 — "Remembers"

## Purpose

buddy has memory and a model configuration architecture. It recalls past conversations, learns user preferences, maintains working context across a session, and supports multiple model slots (chat, embedding) with fallback chains.

## Capabilities

- **Model configuration by concern:** The config schema changes from a single `[provider]` to per-concern model slots: `[models.chat]`, `[models.embedding]`. Each slot holds an ordered list of providers — first is the default, the rest are fallbacks tried in order when the one above is unavailable. This is config-file-only for now (Settings UI comes in V0.4).
- **Local embedding models:** buddy ships with a Rust crate (e.g. `fastembed`) that provides local embedding out of the box. A local model (such as `all-MiniLM-L6-v2`) is set as the default embedding provider on first use — zero external dependencies for memory to work.
- **Embedding dimension tracking:** The vector store records the model name and dimension count alongside stored vectors. On model change, buddy detects dimension mismatch and prompts the user to re-embed. Vectors are never mixed across dimensions — queries block until migration completes or the user opts to discard old memories. Source text is always stored alongside vectors so re-embedding is lossless.
- **Fallback logic (backend):** When the primary model for a slot is unreachable (connection error, timeout, rate limit), buddy tries the next provider in the slot's list. Transparent to the user except for a brief status indicator.
- **Short-term working memory:** A structured scratchpad (key-value + free-form notes) that persists within a conversation. The LLM can read/write it via `memory_read` and `memory_write` skills. Cleared when the conversation ends.
- **Long-term memory:** Configurable vector store backend (trait-based, like providers). Ships with two implementations:
  - SQLite + local embedding (zero-config)
  - Optional: connect to an external store (Qdrant, ChromaDB) for users who want it
- **Automatic context retrieval:** On each user message, buddy searches long-term memory for relevant past interactions and injects them as system context. The user can see what was recalled (transparency).
- **`remember` skill:** Explicitly save a fact or preference to long-term memory
- **`recall` skill:** Explicitly search memory for something
- **Warning system:** Structured, non-intrusive warnings surfaced to the frontend when configuration is degraded. Example: if no embedding model is configured, chat still works but memory features are disabled, with a banner: *"An embedding model is required for long-term memory. This conversation will be cleared from memory once the page is closed. To configure an embedding model, edit buddy.toml under [models.embedding]."*
- **User approval flow:** Before `write_file` or any mutating skill executes, the UI shows what will happen and waits for explicit user confirmation. Configurable: always-ask, ask-once-per-skill, or trust (per-skill).

## Key Architectural Decisions

- **Single config file.** `buddy.toml` remains the sole configuration file. Sections provide separation of concerns. No file splitting until V0.4's Settings UI proves it necessary.
- **Config schema (example):**
  ```toml
  [server]
  host = "127.0.0.1"
  port = 3000

  [models.chat]
  providers = [
    { type = "lmstudio", model = "deepseek-coder", endpoint = "http://10.0.0.3:1234/v1" },
    { type = "openai", model = "gpt-4o", endpoint = "https://api.openai.com/v1", api_key_env = "OPENAI_API_KEY" },
  ]

  [models.embedding]
  providers = [
    { type = "local", model = "all-MiniLM-L6-v2" },
  ]

  [skills.read_file]
  allowed_directories = ["/home/user/projects"]
  ```
- The `VectorStore` trait is deliberately minimal: `store(id, embedding, metadata)`, `search(embedding, limit) -> Vec<Result>`. Two implementations are enough to validate the boundary.
- Working memory is a skill, not a special subsystem. The LLM decides when to use it. This avoids complex heuristics about what to remember.
- Long-term memory retrieval is automatic but visible. The user can disable it per-conversation or globally. No hidden context injection.
- The approval flow is implemented as middleware in the skill execution pipeline, not bolted onto individual skills. Every skill declares its `PermissionLevel` (read-only, mutating, network). The middleware enforces the user's policy.
- **Embedding migration:** When a user switches embedding models with different dimensions, the vector store detects the mismatch and prompts for re-embedding. The source text is always stored alongside vectors, so re-embedding is lossless — just slow for large stores. Never mix dimensions in the same store.

## Intentionally Not Included

- No Settings UI (config is file-only — UI comes in V0.4)
- No drag-and-drop model priority (V0.4)
- No skills management UI (V0.4)
- No multi-agent or task delegation
- No Telegram/WhatsApp
- No cross-conversation "projects" or workspaces

## Sets Up V0.4 By

The model-slot architecture and fallback system provide the backend that the V0.4 Settings UI will manage. The warning system provides the user-facing feedback patterns that V0.4 will expand. The approval flow and permission model are prerequisites for running buddy on non-localhost interfaces later.
