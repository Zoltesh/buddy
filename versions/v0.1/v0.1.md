# V0.1 — "Hello, World"

## Purpose

Prove the architecture. A self-hosted chat UI that talks to one LLM provider.

## Capabilities

- Rust backend with a single HTTP/WebSocket endpoint for conversation
- Svelte + Tailwind web UI with streaming responses
- One LLM provider wired up (e.g., OpenAI-compatible API) via a `Provider` trait
- Single-user, single-conversation (no history persistence)
- Configuration via a single TOML file (API key, model, endpoint URL)

## Key Architectural Decisions

- Define the `Provider` trait boundary early: `async fn complete(messages, config) -> Stream<Token>`. Every future provider implements this. Getting this trait right is the most important work in V0.1.
- Backend exposes a clean internal API (request/response types) that the web UI consumes. This is the same API that Telegram/WhatsApp will use later — design it as if those clients already exist.
- No auth yet (single-user, localhost-only by default). But the server binds to `127.0.0.1`, not `0.0.0.0` — secure by default.
- Message types are defined as an enum (`User`, `Assistant`, `System`, `ToolCall`, `ToolResult`) from day one, even though tools don't exist yet. This avoids a painful migration later.

## Intentionally Not Included

- No skills, tools, or function calling
- No conversation persistence or memory
- No multi-provider support (trait exists, only one impl)
- No Telegram/WhatsApp
- No authentication or multi-user

## Sets Up V0.2 By

Establishing the provider abstraction, the internal API shape, and the message type system that tool calls will plug into.
